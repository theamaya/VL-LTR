/home/amaya.dharmasiri/.conda/envs/VL-LTR/lib/python3.10/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
| distributed init (rank 2, word 4): env://
| distributed init (rank 3, word 4): env://
| distributed init (rank 0, word 4): env://
| distributed init (rank 1, word 4): env://
Namespace(fp32_resume=False, batch_size=256, epochs=100, config='/home/amaya.dharmasiri/LongTail/VL-LTR/configs/inat/pretrain/inat_pretrain_r50.py', pretrained_clip='pretrained/RN50.pt', txt_embed_path=None, vis_backbone_path=None, two_branch=False, debug=False, desc_path='/l/users/amaya.dharmasiri/data/iNat', context_length=75, sent_length=64, cls_token_length=1, loss_type='smoothCE', pretrain_cvlp=True, pretrain_cvlp_path='', model='CVLP_r50', input_size=224, drop=0.0, drop_path=0.1, img_grad=True, train_mode=True, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.05, sched='cosine', lr=0.0005, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, text_lr=0, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=False, clip_ms=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.0, cutmix=0.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', teacher_model=None, teacher_path=None, distillation_type='logits', distillation_alpha=0, distillation_beta=0.5, distillation_tau=1.0, distillation_training_mode=False, finetune='', pretrained=False, weight_sample=True, use_sqrt_freq=True, nb_classes=1000, data_path='/l/users/amaya.dharmasiri/data/imagenet', data_set='INAT', inat_category='name', output_dir='checkpoints/inat_pretrain_r50', device='cuda', seed=0, resume='checkpoints/inat_pretrain_r50/checkpoint.pth', start_epoch=0, eval=False, test=False, test_p=False, select=False, eval_pretrain=True, ensemble=False, dist_eval=False, num_workers=4, pin_mem=True, drop_last=True, world_size=4, dist_url='env://', port=8666, local_rank=0, rank=0, gpu=0, distributed=True, dist_backend='nccl')
using clip text tokens splitted by sentence
using clip text tokens splitted by sentence
Creating model: CVLP_r50
loaded pretrained clip.
<All keys matched successfully>
number of params: 102007137
using loss:  LabelSmoothingCrossEntropy()
loaded pretrained clip.
<All keys matched successfully>
using loss:  <class 'losses.PretrainSentLoss'>
Start training for 100 epochs
Epoch: [0]  [  0/427]  eta: 3:10:45  lr: 0.000001  distill_loss: 8.6962 (8.6962)  img_acc1: 0.7812 (0.7812)  text_acc1: 1.9531 (1.9531)  loss: 9.5281 (9.5281)  time: 26.8046  data: 13.2709  max mem: 21272
Epoch: [0]  [ 10/427]  eta: 0:23:49  lr: 0.000001  distill_loss: 8.7571 (8.7349)  img_acc1: 0.8789 (0.9411)  text_acc1: 1.4648 (1.3672)  loss: 9.5439 (9.5630)  time: 3.4274  data: 1.6173  max mem: 22456
Epoch: [0]  [ 20/427]  eta: 0:18:14  lr: 0.000001  distill_loss: 8.2943 (8.0755)  img_acc1: 0.8789 (0.9719)  text_acc1: 1.4648 (1.4090)  loss: 9.1148 (8.9646)  time: 1.4832  data: 0.6928  max mem: 22456
Epoch: [0]  [ 30/427]  eta: 0:17:15  lr: 0.000001  distill_loss: 6.6845 (7.5703)  img_acc1: 1.0742 (1.0805)  text_acc1: 1.3672 (1.3735)  loss: 7.5466 (8.4335)  time: 2.1579  data: 1.1777  max mem: 22456
Epoch: [0]  [ 40/427]  eta: 0:15:35  lr: 0.000001  distill_loss: 6.3698 (7.2571)  img_acc1: 1.4648 (1.1909)  text_acc1: 1.4648 (1.4601)  loss: 7.0922 (8.0718)  time: 2.1308  data: 1.0553  max mem: 22456
Epoch: [0]  [ 50/427]  eta: 0:15:05  lr: 0.000001  distill_loss: 6.1843 (7.0400)  img_acc1: 1.2695 (1.1910)  text_acc1: 1.5625 (1.4706)  loss: 6.8026 (7.8086)  time: 2.0820  data: 0.6031  max mem: 22456
Epoch: [0]  [ 60/427]  eta: 0:14:05  lr: 0.000001  distill_loss: 6.1008 (6.8814)  img_acc1: 1.3672 (1.2871)  text_acc1: 1.6602 (1.5081)  loss: 6.6436 (7.6125)  time: 2.0752  data: 0.4616  max mem: 22456
Epoch: [0]  [ 70/427]  eta: 0:13:47  lr: 0.000001  distill_loss: 6.0458 (6.7597)  img_acc1: 1.7578 (1.3782)  text_acc1: 1.6602 (1.5199)  loss: 6.5729 (7.4608)  time: 2.1078  data: 0.5127  max mem: 22456
Epoch: [0]  [ 80/427]  eta: 0:13:03  lr: 0.000001  distill_loss: 5.9777 (6.6605)  img_acc1: 1.9531 (1.4624)  text_acc1: 1.7578 (1.5770)  loss: 6.4995 (7.3402)  time: 2.1141  data: 0.5237  max mem: 22456
Epoch: [0]  [ 90/427]  eta: 0:12:37  lr: 0.000001  distill_loss: 5.9116 (6.5764)  img_acc1: 2.0508 (1.5217)  text_acc1: 2.0508 (1.6226)  loss: 6.4514 (7.2407)  time: 1.9992  data: 0.6955  max mem: 22456
Epoch: [0]  [100/427]  eta: 0:11:56  lr: 0.000001  distill_loss: 5.8727 (6.5053)  img_acc1: 2.0508 (1.5818)  text_acc1: 2.0508 (1.6631)  loss: 6.4153 (7.1571)  time: 1.9142  data: 0.8786  max mem: 22456
Epoch: [0]  [110/427]  eta: 0:11:34  lr: 0.000001  distill_loss: 5.8357 (6.4424)  img_acc1: 2.1484 (1.6285)  text_acc1: 1.9531 (1.6927)  loss: 6.3765 (7.0847)  time: 1.9311  data: 1.1720  max mem: 22456
Epoch: [0]  [120/427]  eta: 0:11:01  lr: 0.000001  distill_loss: 5.7883 (6.3871)  img_acc1: 2.1484 (1.6941)  text_acc1: 2.3438 (1.7489)  loss: 6.3352 (7.0211)  time: 1.9749  data: 1.3294  max mem: 22456
Epoch: [0]  [130/427]  eta: 0:10:32  lr: 0.000001  distill_loss: 5.7376 (6.3358)  img_acc1: 2.5391 (1.7600)  text_acc1: 2.3438 (1.8107)  loss: 6.2895 (6.9636)  time: 1.7869  data: 1.1571  max mem: 22456
Epoch: [0]  [140/427]  eta: 0:10:02  lr: 0.000001  distill_loss: 5.7004 (6.2915)  img_acc1: 2.5391 (1.8084)  text_acc1: 2.6367 (1.8915)  loss: 6.2710 (6.9147)  time: 1.7599  data: 1.1107  max mem: 22456
Epoch: [0]  [150/427]  eta: 0:09:37  lr: 0.000001  distill_loss: 5.6899 (6.2496)  img_acc1: 2.6367 (1.8865)  text_acc1: 2.8320 (1.9570)  loss: 6.2520 (6.8692)  time: 1.7957  data: 1.1837  max mem: 22456
Epoch: [0]  [160/427]  eta: 0:09:07  lr: 0.000001  distill_loss: 5.6604 (6.2134)  img_acc1: 2.7344 (1.9367)  text_acc1: 2.7344 (2.0077)  loss: 6.2195 (6.8294)  time: 1.7283  data: 1.1752  max mem: 22456
Epoch: [0]  [170/427]  eta: 0:08:46  lr: 0.000001  distill_loss: 5.6628 (6.1803)  img_acc1: 2.4414 (1.9794)  text_acc1: 2.5391 (2.0428)  loss: 6.2100 (6.7928)  time: 1.7548  data: 1.2043  max mem: 22456
Epoch: [0]  [180/427]  eta: 0:08:17  lr: 0.000001  distill_loss: 5.6141 (6.1483)  img_acc1: 2.7344 (2.0438)  text_acc1: 2.8320 (2.1198)  loss: 6.1737 (6.7577)  time: 1.7121  data: 1.1607  max mem: 22456
Epoch: [0]  [190/427]  eta: 0:07:57  lr: 0.000001  distill_loss: 5.5930 (6.1190)  img_acc1: 3.0273 (2.0948)  text_acc1: 3.3203 (2.1801)  loss: 6.1526 (6.7259)  time: 1.7231  data: 1.1079  max mem: 22456
Epoch: [0]  [200/427]  eta: 0:07:31  lr: 0.000001  distill_loss: 5.5808 (6.0919)  img_acc1: 3.0273 (2.1392)  text_acc1: 3.1250 (2.2262)  loss: 6.1464 (6.6969)  time: 1.7357  data: 1.0851  max mem: 22456
Epoch: [0]  [210/427]  eta: 0:07:11  lr: 0.000001  distill_loss: 5.5611 (6.0664)  img_acc1: 3.0273 (2.1887)  text_acc1: 3.1250 (2.2813)  loss: 6.1145 (6.6694)  time: 1.7761  data: 1.1890  max mem: 22456
Epoch: [0]  [220/427]  eta: 0:06:48  lr: 0.000001  distill_loss: 5.5305 (6.0414)  img_acc1: 2.9297 (2.2236)  text_acc1: 3.3203 (2.3265)  loss: 6.1079 (6.6435)  time: 1.8200  data: 1.2687  max mem: 22456
Epoch: [0]  [230/427]  eta: 0:06:27  lr: 0.000001  distill_loss: 5.5087 (6.0183)  img_acc1: 3.2227 (2.2740)  text_acc1: 3.5156 (2.3991)  loss: 6.0843 (6.6188)  time: 1.6970  data: 1.1489  max mem: 22456
Epoch: [0]  [240/427]  eta: 0:06:03  lr: 0.000001  distill_loss: 5.4960 (5.9966)  img_acc1: 3.2227 (2.3174)  text_acc1: 4.0039 (2.4568)  loss: 6.0693 (6.5958)  time: 1.6494  data: 1.0179  max mem: 22456
